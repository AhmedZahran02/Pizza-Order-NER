{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10113990,"sourceType":"datasetVersion","datasetId":6223286},{"sourceId":10194011,"sourceType":"datasetVersion","datasetId":6298724},{"sourceId":10247843,"sourceType":"datasetVersion","datasetId":6301025},{"sourceId":10254649,"sourceType":"datasetVersion","datasetId":6343313}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport re\nimport pickle\nimport numpy as np\nimport random\nfrom torch.utils.data import Dataset, DataLoader\nfrom nltk.stem import PorterStemmer\nstemmer = PorterStemmer()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T21:33:08.189639Z","iopub.execute_input":"2024-12-20T21:33:08.189942Z","iopub.status.idle":"2024-12-20T21:33:08.194193Z","shell.execute_reply.started":"2024-12-20T21:33:08.189919Z","shell.execute_reply":"2024-12-20T21:33:08.193149Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"# Load Data\ndef load_data(file_x, file_y):\n    with open(file_x, 'r') as fx, open(file_y, 'r') as fy:\n        sentences = fx.read().strip().split('\\n')\n        labels = fy.read().strip().split('\\n')\n    return [sentence.split(\",\") for sentence in sentences], [label.split(',') for label in labels]\n\nFolder = \"/kaggle/input/yarabn5ls/OrderLabeler\"\n\nprint(\"Loading Data Started\")\ntrain_sentences, train_labels = load_data(f'{Folder}/x_train.txt', f'{Folder}/y_train.txt')\ntest_sentences, test_labels = load_data(f'{Folder}/x_dev.txt', f'{Folder}/y_dev.txt')\n\n\ntrain_sentences = train_sentences\ntrain_labels = train_labels\nsentences = train_sentences + test_sentences\nlabels = train_labels + test_labels\n\nentries = [ (x, y) for x, y in zip(sentences, labels) ]\nrandom.shuffle(entries)\n\ntrain = entries[ : int(0.25 * len(entries))]\ntest = entries[int(0.95 * len(entries)) : ]\n\ntrain_sentences = []\ntrain_labels = []\nfor x, y in train:\n    train_sentences.append(x); train_labels.append(y)\ntest_sentences = []\ntest_labels = []\nfor x, y in test:\n   test_sentences.append(x); test_labels.append(y)\n\n\nprint(\"Loading Data Done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T21:27:41.991437Z","iopub.execute_input":"2024-12-20T21:27:41.991730Z","iopub.status.idle":"2024-12-20T21:28:20.621010Z","shell.execute_reply.started":"2024-12-20T21:27:41.991707Z","shell.execute_reply":"2024-12-20T21:28:20.620196Z"}},"outputs":[{"name":"stdout","text":"Loading Data Started\nLoading Data Done\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"print(train_sentences[0])\nprint(train_labels[0])\n\nprint(test_sentences[0])\nprint(test_labels[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T21:28:20.621937Z","iopub.execute_input":"2024-12-20T21:28:20.622250Z","iopub.status.idle":"2024-12-20T21:28:20.626835Z","shell.execute_reply.started":"2024-12-20T21:28:20.622196Z","shell.execute_reply":"2024-12-20T21:28:20.626112Z"}},"outputs":[{"name":"stdout","text":"['sm_num', 'pizzas', 'with', 'pepperoni', 'and', 'sm_num', 'pies', 'with', 'gold', 'leaf', 'and', 'low', 'fat', 'cheese']\n['B_PIZZAORDER', 'PIZZAORDER', 'PIZZAORDER', 'E_PIZZAORDER', 'NONE', 'B_PIZZAORDER', 'PIZZAORDER', 'PIZZAORDER', 'PIZZAORDER', 'PIZZAORDER', 'PIZZAORDER', 'PIZZAORDER', 'PIZZAORDER', 'E_PIZZAORDER']\n[\"i'd\", 'like', 'a', 'pizza', 'with', 'cheddar', 'camembert', 'and', 'caramelized', 'onions', 'hold', 'the', 'fruit']\n['NONE', 'NONE', 'B_PIZZAORDER', 'PIZZAORDER', 'PIZZAORDER', 'PIZZAORDER', 'PIZZAORDER', 'PIZZAORDER', 'PIZZAORDER', 'PIZZAORDER', 'PIZZAORDER', 'PIZZAORDER', 'E_PIZZAORDER']\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"from gensim.models import Word2Vec\nvocab_model = Word2Vec(\n    sentences=train_sentences,      # Tokenized sentences\n    vector_size=100,                # Size of word vectors\n    window=5,                       # Context window size\n    min_count=1,                    # Minimum word frequency\n    sg=1,                           # Skip-gram (1) or CBOW (0)\n    epochs=10                       # Number of training epochs\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T21:31:11.573733Z","iopub.execute_input":"2024-12-20T21:31:11.574042Z","iopub.status.idle":"2024-12-20T21:32:45.310008Z","shell.execute_reply.started":"2024-12-20T21:31:11.574014Z","shell.execute_reply":"2024-12-20T21:32:45.309081Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"word2idx = {word: idx + 2 for idx, word in enumerate(vocab_model.wv.index_to_key)}  # Start indices from 1\nword2idx['<PAD>'] = 0  # Padding token\nword2idx['<UNK>'] = 0  # Padding token\nidx_to_word = {idx: word for word, idx in vocab.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T21:35:01.831572Z","iopub.execute_input":"2024-12-20T21:35:01.831903Z","iopub.status.idle":"2024-12-20T21:35:01.905738Z","shell.execute_reply.started":"2024-12-20T21:35:01.831873Z","shell.execute_reply":"2024-12-20T21:35:01.905023Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"embedding_matrix = np.zeros((len(word2idx), embedding_dim))\nfor word, idx in word2idx.items():\n    if word in vocab_model.wv:\n        embedding_matrix[idx] = vocab_model.wv[word]\nembedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T21:35:03.847867Z","iopub.execute_input":"2024-12-20T21:35:03.848163Z","iopub.status.idle":"2024-12-20T21:35:04.179969Z","shell.execute_reply.started":"2024-12-20T21:35:03.848139Z","shell.execute_reply":"2024-12-20T21:35:04.178936Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"# Build Vocabulary\nFREQ_THRESH = 0\n\ndef build_vocab(sentences):\n    vocab = set()\n    with open(f\"{Folder}/vocabulary.txt\", \"r\") as fv:\n        for line in fv: \n            voc = line.strip()\n            vocab.add(voc)\n    word2idx = {word: idx + 2 for idx, word in enumerate(sorted(vocab))}\n    word2idx['<PAD>'] = 0\n    word2idx['<UNK>'] = 1\n    return word2idx\n\ndef build_label_vocab(labels):\n    vocab = {label for label_list in labels for label in label_list}\n    label2idx = {label: idx for idx, label in enumerate(sorted(vocab))}\n    return label2idx\n\nprint(\"Building Vocab Started\")\n#word2idx = build_vocab(train_sentences + test_sentences)\nlabel2idx = build_label_vocab(train_labels + test_labels)\nidx2label = {idx: label for label, idx in label2idx.items()}\nprint(\"Building Vocab Done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T21:35:07.089322Z","iopub.execute_input":"2024-12-20T21:35:07.089649Z","iopub.status.idle":"2024-12-20T21:35:07.896715Z","shell.execute_reply.started":"2024-12-20T21:35:07.089621Z","shell.execute_reply":"2024-12-20T21:35:07.895979Z"}},"outputs":[{"name":"stdout","text":"Building Vocab Started\nBuilding Vocab Done\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"print(label2idx)\nprint(len(word2idx))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T21:35:09.335039Z","iopub.execute_input":"2024-12-20T21:35:09.335389Z","iopub.status.idle":"2024-12-20T21:35:09.339936Z","shell.execute_reply.started":"2024-12-20T21:35:09.335358Z","shell.execute_reply":"2024-12-20T21:35:09.339008Z"}},"outputs":[{"name":"stdout","text":"{'B_DRINKORDER': 0, 'B_PIZZAORDER': 1, 'DRINKORDER': 2, 'E_DRINKORDER': 3, 'E_PIZZAORDER': 4, 'NONE': 5, 'PIZZA': 6, 'PIZZAORDER': 7}\n96056\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"# Prepare Dataset\nclass SequenceDataset(Dataset):\n    def __init__(self, sentences, labels, word2idx, label2idx, max_len=50):\n        self.sentences = [[word2idx.get(word, word2idx['<UNK>']) for word in sentence] for sentence in sentences]\n        self.labels = [[label2idx[label] for label in label_list] for label_list in labels]\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.sentences)\n\n    def __getitem__(self, idx):\n        sentence = self.sentences[idx]\n        label = self.labels[idx]\n        sentence = sentence[:self.max_len] + [0] * (self.max_len - len(sentence))\n        label = label[:self.max_len] + [label2idx['NONE']] * (self.max_len - len(label))\n        return torch.tensor(sentence), torch.tensor(label)\n\nprint(\"Preparing Dataset Started\")\ntrain_dataset = SequenceDataset(train_sentences, train_labels, word2idx, label2idx)\ntest_dataset = SequenceDataset(test_sentences, test_labels, word2idx, label2idx)\n\ntrain_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=1024)\nprint(\"Preparing Dataset Done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T21:35:12.565741Z","iopub.execute_input":"2024-12-20T21:35:12.566024Z","iopub.status.idle":"2024-12-20T21:35:15.914342Z","shell.execute_reply.started":"2024-12-20T21:35:12.566002Z","shell.execute_reply":"2024-12-20T21:35:15.913581Z"}},"outputs":[{"name":"stdout","text":"Preparing Dataset Started\nPreparing Dataset Done\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"# Define Model\nclass RNNSequenceLabeling(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n\n        global embedding_matrix\n        super(RNNSequenceLabeling, self).__init__()\n        self.vocab_size = vocab_size\n        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, padding_idx=word2idx['<PAD>'])\n        \n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, \n                            num_layers=2, \n                            bidirectional=True, \n                            batch_first=True)\n        self.lstm_dropout = nn.Dropout(p=0.25)\n        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n\n    def forward(self, x):\n        #x = x.long()\n        #one_hot_x = F.one_hot(x, num_classes=self.vocab_size)\n        #one_hot_x = one_hot_x.float()\n\n        embedded = self.embedding(x)\n        \n        lstm_out, _ = self.lstm(embedded)\n        lstm_out = self.lstm_dropout(lstm_out)\n\n        predicted = self.fc(lstm_out)\n        return predicted","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T21:37:34.739860Z","iopub.execute_input":"2024-12-20T21:37:34.740188Z","iopub.status.idle":"2024-12-20T21:37:34.745955Z","shell.execute_reply.started":"2024-12-20T21:37:34.740158Z","shell.execute_reply":"2024-12-20T21:37:34.745012Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass RNNSequenceLabeling(nn.Module):\n    def __init__(self, input_dim, embedding_size, hidden_dim, output_dim):\n        super(RNNSequenceLabeling, self).__init__()\n        global embedding_matrix\n\n        self.vocab_size = input_dim\n        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, padding_idx=word2idx['<PAD>'])\n        self.rnn = nn.RNN(embedding_size, hidden_dim, \n                          num_layers=2, \n                          bidirectional=False, \n                          batch_first=True)\n        self.rnn_dropout = nn.Dropout(p=0.25)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        #x = x.long()\n        #one_hot_x = F.one_hot(x, num_classes=self.vocab_size)\n        #one_hot_x = one_hot_x.float()\n\n        embedded = self.embedding(x)\n        rnn_out, _ = self.rnn(embedded)\n        rnn_out = self.rnn_dropout(rnn_out)\n        predicted = self.fc(rnn_out)\n        return predicted","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T21:51:44.534633Z","iopub.execute_input":"2024-12-20T21:51:44.534920Z","iopub.status.idle":"2024-12-20T21:51:44.540638Z","shell.execute_reply.started":"2024-12-20T21:51:44.534897Z","shell.execute_reply":"2024-12-20T21:51:44.539731Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"# Training\nprint(\"Training Started\")\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nvocab_size = len(word2idx)\nembedding_dim = 100\nhidden_dim = 128\noutput_dim = len(label2idx)\n\nmodel = RNNSequenceLabeling(vocab_size, embedding_dim, hidden_dim, output_dim).to(device)\n#model = RNNSequenceLabeling(vocab_size, hidden_dim, output_dim).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    for sentences, labels in train_loader:\n        sentences, labels = sentences.to(device), labels.to(device)\n        optimizer.zero_grad()\n        predictions = model(sentences)\n\n        loss = criterion(predictions.view(-1, output_dim), labels.view(-1)) \n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader):.25f}\")\nprint(\"Training Done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T21:51:52.743694Z","iopub.execute_input":"2024-12-20T21:51:52.743984Z","iopub.status.idle":"2024-12-20T21:55:54.709455Z","shell.execute_reply.started":"2024-12-20T21:51:52.743959Z","shell.execute_reply":"2024-12-20T21:55:54.708653Z"}},"outputs":[{"name":"stdout","text":"Training Started\nEpoch 1/10, Loss: 0.0874930511477092914818243\nEpoch 2/10, Loss: 0.0325627161391700281622974\nEpoch 3/10, Loss: 0.0291640496486797913544553\nEpoch 4/10, Loss: 0.0276005555829033251413751\nEpoch 5/10, Loss: 0.0266389916899303601238724\nEpoch 6/10, Loss: 0.0259778965202470610296182\nEpoch 7/10, Loss: 0.0254920858703553687707455\nEpoch 8/10, Loss: 0.0251200397623081994602501\nEpoch 9/10, Loss: 0.0248817181618263306985828\nEpoch 10/10, Loss: 0.0245961157170434784047242\nTraining Done\n","output_type":"stream"}],"execution_count":59},{"cell_type":"code","source":"# Saving Results\nmodel.to(\"cpu\")\n\nmodelFile = open(\"model\", \"wb\")\nword2idxFile = open(\"word2idx\", \"wb\")\nidx2labelFile = open(\"label2idx.idx\", \"wb\")\n\npickle.dump(model, modelFile)\npickle.dump(word2idx, word2idxFile)\npickle.dump(idx2label, idx2labelFile)\n\nmodelFile.close()\nword2idxFile.close()\nidx2labelFile.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T18:13:59.625325Z","iopub.execute_input":"2024-12-20T18:13:59.625634Z","iopub.status.idle":"2024-12-20T18:13:59.645405Z","shell.execute_reply.started":"2024-12-20T18:13:59.625611Z","shell.execute_reply":"2024-12-20T18:13:59.644765Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# Evaluation\nfrom sklearn.metrics import classification_report\nall_predictions = []\nall_labels = []\ndef evaluate(model, loader):\n    global label2idx, idx2word\n    model.eval()\n    total, correct = 0, 0\n    with torch.no_grad():\n        for sentences, labels in loader:\n            sentences, labels = sentences.to(device), labels.to(device)\n            predictions = model(sentences).argmax(dim=-1)\n            total += labels.numel()\n            correct += (predictions == labels).sum().item()\n            all_predictions.extend(predictions.view(-1).cpu().numpy())\n            all_labels.extend(labels.view(-1).cpu().numpy())\n                    \n    return correct / total\naccuracy = evaluate(model, test_loader)\nreport = classification_report(all_labels, all_predictions, target_names=label2idx.keys())\n\nprint(\"Testing Started\")\nprint(f\"Test Accuracy: {accuracy * 100:.2f}%\")\nprint(\"\\nClassification Report:\")\nprint(report)\nprint(\"Testing Done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T22:26:25.133165Z","iopub.execute_input":"2024-12-20T22:26:25.133707Z","iopub.status.idle":"2024-12-20T22:26:52.396042Z","shell.execute_reply.started":"2024-12-20T22:26:25.133674Z","shell.execute_reply":"2024-12-20T22:26:52.394530Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Testing Started\nTest Accuracy: 98.77%\n\nClassification Report:\n              precision    recall  f1-score   support\n\nB_DRINKORDER       0.90      0.74      0.81     92097\nB_PIZZAORDER       0.83      0.94      0.88    124067\n  DRINKORDER       0.98      0.96      0.97    163200\nE_DRINKORDER       0.96      0.98      0.97     92097\nE_PIZZAORDER       0.90      0.90      0.90    124917\n        NONE       1.00      1.00      1.00   4850071\n       PIZZA       0.00      0.00      0.00         1\n  PIZZAORDER       0.98      0.97      0.97    695550\n\n    accuracy                           0.99   6142000\n   macro avg       0.82      0.81      0.81   6142000\nweighted avg       0.99      0.99      0.99   6142000\n\nTesting Done\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":60}]}